<!DOCTYPE html>


<html lang="en">
  

    <head>
      <meta charset="utf-8" />
       
      <meta name="keywords" content="wgj&#39;s博客,大学课程知识,计算机课程知识,算法知识,实验研究,论文写作,计算机视觉,CV,CSU,xjtu" />
       
      <meta name="description" content="本文介绍Transformer中涉及到的方法和模型，并作简要介绍。" />
      
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>Transformer基础 |  Wgj&#39;s blog</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <script src="https://cdn.staticfile.org/mermaid/8.14.0/mermaid.min.js"></script>
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    <link rel="alternate" href="/atom.xml" title="Wgj's blog" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
  </html>
</html>


<body>
  <div id="app">
    
      <canvas class="fireworks"></canvas>
      <style>
        .fireworks {
          position: fixed;
          left: 0;
          top: 0;
          z-index: 99999;
          pointer-events: none;
        }
      </style>
      
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-University/AI/ModelLearning/ChatGPT/Transformer基础"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  Transformer基础
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/10/23/University/AI/ModelLearning/ChatGPT/Transformer%E5%9F%BA%E7%A1%80/" class="article-date">
  <time datetime="2023-10-23T08:51:34.922Z" itemprop="datePublished">2023-10-23</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%A0%BC%E7%89%A9%E8%87%B4%E7%9F%A5/">格物致知</a> / <a class="article-category-link" href="/categories/%E6%A0%BC%E7%89%A9%E8%87%B4%E7%9F%A5/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/">经典模型</a> / <a class="article-category-link" href="/categories/%E6%A0%BC%E7%89%A9%E8%87%B4%E7%9F%A5/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/Transformer/">Transformer</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">3.2k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">12 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h1>Transformer</h1>
<blockquote>
<p>详细见参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/338817680">https://zhuanlan.zhihu.com/p/338817680</a></p>
</blockquote>
<h3 id="transformer整体概述">Transformer整体概述</h3>
<p>用于中英文翻译的Transformer整体结构：</p>
<img src="https://pic4.zhimg.com/v2-4544255f3f24b7af1e520684ae38403f_r.jpg" alt="img" style="zoom:50%;" />
<p><strong>Transformer 由 Encoder 和 Decoder 两个部分组成</strong>，Encoder 和 Decoder 都包含 6 个 block。</p>
<p><strong>第一步：<strong>获取输入句子的每一个单词的表示向量 <strong>X</strong>，<strong>X</strong>由单词的 Embedding（Embedding就是从原始数据提取出来的</strong>Feature</strong>） 和单词位置的 Embedding 相加得到。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><mark>思考：单词的 Embedding和位置的 Embedding如何获得？</mark></p>
</li>
</ul>
<p><img src="https://pic4.zhimg.com/v2-7dd39c44b0ae45d31a3ae7f39d3f883f_r.jpg" alt="img"></p>
<p><strong>第二步：<strong>将得到的单词表示向量矩阵</strong>x</strong>(<strong>X<sub>n*d</sub></strong>，n表示的是句子中单词的个数，d表示的每个单词向量的维度)输入到<strong>Encoder</strong>中，经过6个 Encoder block 后可以得到句子所有单词的编码信息矩阵 <strong>C</strong>。每一个 Encoder block 输出的矩阵维度与输入完全一致。</p>
<img src="https://pic3.zhimg.com/80/v2-45db05405cb96248aff98ee07a565baa_1440w.webp" alt="img" style="zoom:33%;" />
<p><strong>第三步</strong>：将 <strong>Encoder</strong> 输出的编码信息矩阵 <strong>C</strong>传递到 <strong>Decoder</strong> 中，Decoder 依次会根据当前翻译过的<strong>单词 1~ i</strong> 翻译<strong>下一个单词 i+1</strong>，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 <strong>Mask (掩盖)</strong> 操作遮盖住 i+1 之后的单词。</p>
<p><img src="https://pic2.zhimg.com/v2-5367bd47a2319397317562c0da77e455_r.jpg" alt="img"></p>
<p><strong>Decoder</strong> 接收了 <strong>Encoder</strong> 的编码矩阵 <strong>C</strong>，然后首先输入一个翻译开始符 “<Begin>”，预测第一个单词 “I”；然后输入翻译开始符 “<Begin>” 和单词 “I”，预测单词 “have”，以此类推，如下图所示。</p>
<p><img src="https://pic1.zhimg.com/v2-4616451fe8aa59b2df2ead30fa31dc98_r.jpg" alt="img"></p>
<h4 id="transformer详细架构">Transformer详细架构</h4>
<img src="https://pic4.zhimg.com/v2-f6380627207ff4d1e72addfafeaff0bb_r.jpg" alt="img" style="zoom:50%;" />
<p>上图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为 <strong>Multi-Head Attention</strong>，是由多个 <strong>Self-Attention</strong>组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 <strong>Add &amp; Norm</strong> 层，<strong>Add</strong> 表示<u>残差连接</u> (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行<strong>归一化</strong>。</p>
<h5 id="encoder-结构">Encoder 结构</h5>
<img src="https://wangguijie-typora.oss-cn-chengdu.aliyuncs.com/img/image-20231018153045934.png" alt="image-20231018153045934" style="zoom: 33%;" />
<p>Encoder block 结构由 Multi-Head Attention, <strong>Add &amp; Norm, Feed Forward, Add &amp; Norm</strong> 组成。其中Multi-Head Attention详见下面的“注意力机制”部分。</p>
<h6 id="add-norm">Add &amp; Norm</h6>
<p>Add &amp; Norm 层由 Add 和 Norm 两部分组成，其计算公式如下：</p>
<img src="https://pic3.zhimg.com/v2-a4b35db50f882522ee52f61ddd411a5a_r.jpg" alt="img" style="zoom: 80%;" />
<p>其中 <strong>X</strong>表示 Multi-Head Attention 或者 Feed Forward 的输入，MultiHeadAttention(<strong>X</strong>) 和 FeedForward(<strong>X</strong>) 表示输出 (输出与输入 <strong>X</strong> 维度是一样的，所以可以相加)。</p>
<p><strong>Add</strong>指 <strong>X</strong>+MultiHeadAttention(<strong>X</strong>)，是一种<strong>残差连接</strong>，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分，在 ResNet 中经常用到：</p>
<p><img src="https://pic4.zhimg.com/v2-4b3dde965124bd00f9893b05ebcaad0f_r.jpg" alt="img"></p>
<p><strong>Norm</strong>指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的（<strong>归一化</strong>），这样可以加快收敛。</p>
<h6 id="feed-forward">Feed Forward</h6>
<p>Feed Forward 层比较简单，是一个两层的全连接层，第一层的<strong>激活函数</strong>为 <strong>Relu</strong>，第二层<strong>不使用激活函数</strong>，对应的公式如下：</p>
<img src="https://pic2.zhimg.com/v2-47b39ca4cc3cd0be157d6803c8c8e0a1_r.jpg" alt="img" style="zoom:80%;" />
<p><strong>X</strong>是输入，Feed Forward 最终得到的输出矩阵的维度与<strong>X</strong>一致。</p>
<h5 id="decoder-结构">Decoder 结构</h5>
<img src="https://pic3.zhimg.com/v2-f5049e8711c3abe8f8938ced9e7fc3da_r.jpg" alt="img" style="zoom: 50%;" />
<p>Decoder block 结构如下：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>包含两个 Multi-Head Attention 层。</p>
</li>
<li class="lvl-2">
<p>第一个 Multi-Head Attention 层采用了 <strong>Masked</strong> 操作。</p>
</li>
<li class="lvl-2">
<p>第二个 Multi-Head Attention 层的<strong>V，K</strong>矩阵使用 Encoder 的<strong>编码信息矩阵C</strong>进行计算，而<strong>Q</strong>使用上一个 Decoder block 的输出计算。</p>
</li>
<li class="lvl-2">
<p>最后有一个 Softmax 层计算下一个翻译单词的概率。</p>
</li>
</ul>
<h4 id="注意力机制">注意力机制</h4>
<p>Attention 有许多种实现方式，但是最常见的还是 Scaled Dot-product Attention。</p>
<img src="https://transformers.run/assets/img/attention/attention.png" alt="img" style="zoom:50%;" />
<h5 id="self-attention">Self-Attention</h5>
<p>采用Scaled Dot-product Attention实现，在计算的时候需要用到矩阵<strong>Q(查询),K(键值),V(值)</strong>。</p>
<p>Self-Attention 接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而<strong>Q,K,V</strong>正是通过 Self-Attention 的输入进行线性变换得到的。</p>
<p>Self-Attention 的输入用矩阵X进行表示，可以使用线性变阵矩阵<strong>WQ,WK,WV</strong>计算得到<strong>Q,K,V</strong>。计算如下图所示，<strong>注意 X, Q, K, V 的每一行都表示一个单词。</strong></p>
<img src="https://pic3.zhimg.com/80/v2-4f4958704952dcf2c4b652a1cd38f32e_1440w.webp" alt="img" style="zoom: 33%;" />
<p>根据以下公式计算出Self-Attention的输出：</p>
<p><img src="https://pic2.zhimg.com/80/v2-9699a37b96c2b62d22b312b5e1863acd_1440w.webp" alt="img"></p>
<p>公式中计算矩阵<strong>Q</strong>和<strong>K</strong>每一行向量的内积，为了防止内积过大，因此除以**d<sub>k</sub>**的平方根。矩阵Q与K的转置计算得到的矩阵行列维度均为n，n表示的时句子中单词的个数，计算得到的矩阵表示了单词之间的attention强度。</p>
<img src="https://pic2.zhimg.com/80/v2-9caab2c9a00f6872854fb89278f13ee1_1440w.webp" alt="img" style="zoom:50%;" />
<p>得到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1.</p>
<img src="https://pic1.zhimg.com/80/v2-96a3716cf7f112f7beabafb59e84f418_1440w.webp" alt="img" style="zoom:50%;" />
<p>得到 Softmax 矩阵之后可以和<strong>V</strong>相乘，得到最终的输出<strong>Z</strong>。</p>
<img src="https://pic4.zhimg.com/80/v2-7ac99bce83713d568d04e6ecfb31463b_1440w.webp" alt="img" style="zoom:50%;" />
<p>上图中 Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention 系数，最终<strong>单词 1</strong> 的输出 <strong>Z1</strong> 等于所有单词 i 的值**V<sub>i</sub>**根据 attention 系数的比例加在一起得到，如下图所示：</p>
<img src="https://pic3.zhimg.com/80/v2-27822b2292cd6c38357803093bea5d0e_1440w.webp" alt="img" style="zoom: 80%;" />
<h5 id="multi-head-attention">Multi-Head Attention</h5>
<p>Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。</p>
<img src="https://pic2.zhimg.com/v2-b0ea8f5b639786f98330f70405e94a75_r.jpg" alt="img" style="zoom:33%;" />
<p>Multi-Head Attention 包含多个 Self-Attention 层，首先将输入<strong>X</strong>分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵<strong>Z</strong>。如下图是 h=8 时候的情况，此时会得到 8 个输出矩阵<strong>Z</strong>。</p>
<img src="https://pic1.zhimg.com/v2-6bdaf739fd6b827b2087b4e151c560f4_r.jpg" alt="img" style="zoom: 33%;" />
<p>得到 8 个输出矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Z</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">Z_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Z</mi><mn>8</mn></msub></mrow><annotation encoding="application/x-tex">Z_8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 之后，Multi-Head Attention 将它们拼接在一起 <strong>(Concat)</strong>，然后传入一个<strong>Linear</strong>层，得到 Multi-Head Attention 最终的输出<strong>Z</strong>。</p>
<img src="https://pic4.zhimg.com/v2-35d78d9aa9150ae4babd0ea6aa68d113_r.jpg" alt="img" style="zoom:67%;" />
<p>可以看到 Multi-Head Attention 输出的矩阵<strong>Z</strong>与其输入的矩阵<strong>X</strong>的<strong>维度是一样的</strong>。</p>
<h3 id="transformer涉及到的相关知识">Transformer涉及到的相关知识</h3>
<blockquote>
<p>batch norm与layer norm参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/74516930">https://zhuanlan.zhihu.com/p/74516930</a></p>
</blockquote>
<h4 id="batch-norm">batch norm</h4>
<p>Batch 顾名思义是对一个batch进行操作。假设我们有 10行 3列 的数据，即我们的batchsize = 10，每一行数据有三个特征，假设这三个特征是【身高、体重、年龄】。那么BN是针对每一列（特征）进行缩放，例如算出【身高】的均值与方差，再对身高这一列的10个数据进行缩放。体重和年龄同理。这是一种**“列缩放”**。</p>
<h4 id="layer-norm">layer norm</h4>
<p>layer方向相反，它针对的是每一行进行缩放。即只看一笔数据，算出这笔所有特征的均值与方差再缩放。这是一种**“行缩放”**。</p>
<blockquote>
<p>可以发现，针对例子“10行 3列 的数据”：layer normalization 对所有的特征进行缩放，这显得很没道理。我们算出一行这【身高、体重、年龄】三个特征的均值方差并对其进行缩放，事实上会因为特征的量纲不同而产生很大的影响。但是BN则没有这个影响，因为BN是对一列进行缩放，一列的量纲单位都是相同的。</p>
</blockquote>
<h4 id="为什么nlp中常用layer-norm？">为什么NLP中常用layer norm？</h4>
<p>如果我们将一批文本组成一个batch，那么BN的操作方向是，对每句话的<strong>第一个</strong>词进行操作。但语言文本的复杂性是很高的，任何一个词都有可能放在初始位置，且词序可能并不影响我们对句子的理解。而BN是<strong>针对每个位置</strong>进行缩放，这<strong>不符合NLP的规律</strong>。</p>
<p>而LN则是针对一句话进行缩放的，且L<strong>N一般用在第三维度</strong>，如[batchsize, seq_len, dims]中的dims，一般为词向量的维度。这一维度各个特征的量纲应该相同。因此也不会遇到上面因为特征的量纲不同而导致的缩放问题。</p>
<blockquote>
<p>为啥<strong>BN不适合NLP</strong> 是因为NLP模型训练里的每次输入的句子都是多个句子，并且<strong>长度不一</strong>，那么针对每一句的缩放才更加合理，才能表达每个句子之间代表不同的语义表示，这样让模型更加能捕捉句子之间的上下语义关系。如果要用BN，它首先要面临的长度不一的问题。有时候batch size 越小的bn 效果更不好。</p>
</blockquote>
<img src="https://wangguijie-typora.oss-cn-chengdu.aliyuncs.com/img/v2-5a52774dde73a4dc86bcd55a88be5d04_r.jpg" alt="img" style="zoom:50%;" />
<p>**Batch Norm：**在不同样本的同一个词位置上的不同的特征维度上做归一化（按照每一个绿色箭头方向做归一化）；</p>
<p>**Layer Norm：**在同一个样本的不同词位置上对词向量（特征维度）做归一化（按照每一个红色箭头方向做归一化）</p>
<blockquote>
<ul class="lvl-1">
<li class="lvl-2">
<p>Z轴表示的是<strong>词向量</strong>。</p>
</li>
<li class="lvl-2">
<p>张量表示形式：[batchsize, seqlen, embedding dim]</p>
</li>
</ul>
</blockquote>
<h4 id="multi-head-attention的mask操作">Multi-Head Attention的Mask操作</h4>
<p><img src="https://pic1.zhimg.com/v2-4616451fe8aa59b2df2ead30fa31dc98_r.jpg" alt="img"></p>
<p>Decoder 可以在训练的过程中使用 Teacher Forcing 并且并行化训练，即将正确的单词序列 (<Begin> I have a cat) 和对应输出 (I have a cat <end>) 传递到 Decoder。那么在预测第 i 个输出时，就要将第 i+1 之后的单词掩盖住，<strong>注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 &quot;<Begin> I have a cat <end>&quot;。</strong></p>
<p>**第一步：**是 Decoder 的输入矩阵和 <strong>Mask</strong> 矩阵，输入矩阵包含 &quot;<Begin> I have a cat&quot; (0, 1, 2, 3, 4) 五个单词的表示向量，<strong>Mask</strong> 是一个 5×5 的矩阵。在 <strong>Mask</strong> 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。</p>
<p><img src="https://pic1.zhimg.com/80/v2-b26299d383aee0dd42b163e8bda74fc8_720w.webp" alt="img"></p>
<blockquote>
<p><strong>Mask矩阵</strong>相当于：不遮挡（绿色）部分为1，遮挡（黄色）部分为0。</p>
</blockquote>
<p><strong>第二步：<strong>接下来的操作和之前的 Self-Attention 一样，通过输入矩阵</strong>X</strong>计算得到<strong>Q,K,V</strong>矩阵。然后计算<strong>Q</strong>和<strong>K<sup>T</sup><strong>的乘积</strong>QK<sup>T</sup></strong>。</p>
<p><img src="https://pic4.zhimg.com/v2-a63ff9b965595438ed0c0e0547cd3d3b_r.jpg" alt="img"></p>
<p>**第三步：**在得到 <strong>QK<sup>T</sup><strong>之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用</strong>Mask</strong>矩阵遮挡住每一个单词之后的信息，遮挡操作如下：</p>
<p><img src="https://pic2.zhimg.com/v2-35d1c8eae955f6f4b6b3605f7ef00ee1_r.jpg" alt="img"></p>
<p>得到 <strong>Mask</strong> **QK<sup>T</sup>**之后在 <strong>Mask</strong>  **QK<sup>T</sup>**上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。</p>
<p>**第四步：**使用 <strong>Mask</strong>  **QK<sup>T</sup>**与矩阵 <strong>V</strong>相乘，得到输出 <strong>Z</strong>，则单词 1 的输出向量 <strong>Z1</strong> 是只包含单词 1 信息的。</p>
<p><img src="https://pic4.zhimg.com/v2-58f916c806a6981e296a7a699151af87_r.jpg" alt="img"></p>
<p><strong>第五步：<strong>通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵</strong>Z<sub>i</sub></strong>，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出 <strong>Z<sub>i</sub></strong> 然后计算得到第一个 Multi-Head Attention 的输出<strong>Z</strong>，<strong>Z</strong>与输入<strong>X</strong>维度一样。</p>
<h4 id="mlp-multilayer-perceptron">MLP（Multilayer perceptron）</h4>
<p>多层感知器是一种前向结构的人工神经网络，映射一组输入向量到一组输出向量。MLP可以被看作是一个有向图，由多个的节点层所组成，<strong>每一层都全连接到下一层</strong>。除了输入节点，每个节点都是一个带有非线性激活函数的神经元。</p>
<h4 id="position-encodeing">Position Encodeing</h4>
<p>Transformer 中除了单词的 Embedding，还需要使用位置 Embedding 表示单词出现在句子中的位置。**因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。**所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。</p>
<p>位置 Embedding 用 <strong>PE</strong>表示，<strong>PE</strong> 的维度与单词 Embedding 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下：</p>
<img src="https://wangguijie-typora.oss-cn-chengdu.aliyuncs.com/img/v2-8b442ffd03ea0f103e9acc37a1db910a_1440w.webp" alt="img" style="zoom:50%;" />
<p>其中，pos 表示单词在句子中的位置，d 表示 PE的维度 (与词 Embedding 一样)，2i 表示偶数的维度，2i+1 表示奇数维度 (即 2i≤d, 2i+1≤d)。使用这种公式计算 PE 有以下的好处：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>使 PE 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding。</p>
</li>
<li class="lvl-2">
<p>可以让模型容易地计算出相对位置，对于固定长度的间距 k，<strong>PE(pos+k)</strong> 可以用 <strong>PE(pos)</strong> 计算得到。因为 Sin(A+B) = Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)。</p>
</li>
</ul>
<h2 id="transformer-总结">Transformer 总结</h2>
<ul class="lvl-0">
<li class="lvl-2">
<p>Transformer 与 RNN 不同，可以比较好地并行训练。</p>
</li>
<li class="lvl-2">
<p>Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding，否则 Transformer 就是一个词袋模型了。</p>
</li>
<li class="lvl-2">
<p>Transformer 的重点是 Self-Attention 结构，其中用到的 <strong>Q, K, V</strong>矩阵通过输出进行线性变换得到。</p>
</li>
<li class="lvl-2">
<p>Transformer 中 Multi-Head Attention 中有多个 Self-Attention，可以捕获单词之间多种维度上的相关系数 attention score。</p>
</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://wangguijiepedeval.github.io/2023/10/23/University/AI/ModelLearning/ChatGPT/Transformer%E5%9F%BA%E7%A1%80/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/" rel="tag">经典模型</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2023/10/25/University/AI/CV/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%8A%80%E5%B7%A7%EF%BC%9A%E5%BE%AE%E8%B0%83/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            迁移学习技巧：微调
          
        </div>
      </a>
    
    
      <a href="/2023/10/18/University/AI/ModelLearning/ChatGPT/ChatGPT%E5%9F%BA%E7%A1%80/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">ChatGPT基础</div>
      </a>
    
  </nav>

  
   
<div class="gitalk" id="gitalk-container"></div>

<link rel="stylesheet" href="https://cdn.staticfile.org/gitalk/1.7.2/gitalk.min.css">


<script src="https://cdn.staticfile.org/gitalk/1.7.2/gitalk.min.js"></script>


<script src="https://cdn.staticfile.org/blueimp-md5/2.19.0/js/md5.min.js"></script>

<script type="text/javascript">
  var gitalk = new Gitalk({
    clientID: 'ab8e83b45b1c73553e5a',
    clientSecret: 'c34256673ed529723bdea8d206ac6cb5c12e57bb',
    repo: 'wgj_blog_talk',
    owner: 'wangguijiepedeval',
    admin: ['wangguijiepedeval'],
    // id: location.pathname,      // Ensure uniqueness and length less than 50
    id: md5(location.pathname),
    distractionFreeMode: false,  // Facebook-like distraction free mode
    pagerDirection: 'last'
  })

  gitalk.render('gitalk-container')
</script>

  
   
    <script src="https://cdn.staticfile.org/twikoo/1.4.18/twikoo.all.min.js"></script>
    <div id="twikoo" class="twikoo"></div>
    <script>
        twikoo.init({
            envId: ""
        })
    </script>
 
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2023
        <i class="ri-heart-fill heart_icon"></i> Guijie Wang
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Wgj&#39;s blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%97%85%E8%A1%8C/">旅行</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" target="_blank" rel="noopener" href="https://hexo.io/themes/">主题</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/player">播放器</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js"></script>
<script src="https://cdn.staticfile.org/mathjax/2.7.7/config/TeX-AMS-MML_HTMLorMML-full.js"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->
 
    
        <link rel="stylesheet" href="https://cdn.staticfile.org/KaTeX/0.15.1/katex.min.css">
        <script src="https://cdn.staticfile.org/KaTeX/0.15.1/katex.min.js"></script>
        <script src="https://cdn.staticfile.org/KaTeX/0.15.1/contrib/auto-render.min.js"></script>
        
    
 
<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<script src="https://cdn.staticfile.org/animejs/3.2.1/anime.min.js"></script>

<script src="/js/clickBoom1.js"></script>
 
<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->
 
<script src="/js/dz.js"></script>
 
<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>